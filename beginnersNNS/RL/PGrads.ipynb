{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Tennis-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for count in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(250, 160, 3)\n",
      "Action Space: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space: {}\".format(env.observation_space))\n",
    "print(\"Action Space: {}\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f829627dd30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = env.reset()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame):\n",
    "    newframe = frame[45:215,15:150]\n",
    "#     print(newframe.shape)\n",
    "    newframe = cv2.cvtColor(newframe, cv2.COLOR_BGR2GRAY)\n",
    "    newframe = newframe/255.0\n",
    "    \n",
    "    newframe = transform.resize(newframe, [84,84])\n",
    "    return newframe, newframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFMZJREFUeJzt3X2QVNWZx/HvQw/DDO+vIgIKBiISdxFDFGPMuigJ0VSSTSwT87KphBS1m2xiYqoUdyu7Se1urW6lkpgt11oKkzVZ40uMJobKagyBXbIKCL4SXmQUBUbeREAFHGZ6nv3jXm63MMP0zHT37dvn96ma6tOnb9PP5c4z5/S9555j7o6IhGVA2gGISPUp8UUCpMQXCZASXyRASnyRACnxRQKkxBcJUL8S38wWmNkWM2sxs8XlCkpEKsv6OoDHzHLA88B8YCfwBHCtu28sX3giUgkN/XjvhUCLu78IYGb3AB8Fuk38RhvkTQzpx0eKyKm8xWGOeZv1tF1/En8isKPo+U7golO9oYkhXGSX9+MjReRU1vjykrbrT+KXxMwWAYsAmhhc6Y8TkRL05+ReKzC56PmkuO5t3H2Ju89x9zkDGdSPjxORculP4j8BTDezqWbWCHwKeKg8YYlIJfW5q+/uHWb2N8AjQA74kbv/sWyRSb/kZr4TgKNnjkjqmlc/n5TzBw+d9B5riH4dvKOjwtFJ2vr1Hd/dfwP8pkyxiEiVaOSeSIAqflZf0nFkStTFf+3cgUndpPWNp3zPa599DwAjXngrqRuw6qkKRCdpU4svEiC1+HXKc9HgLcsXVba1nfI9I1qOAnBgRnNSN2ZV2UOTGqAWXyRASnyRAKmrX0farnpPUt7/rujQDmgvvG7DhxWevP569PrgwjDqg2c3n/QeqU9q8UUCpMQXCZC6+nXktRmFa/ZDWqMJVg5PLNyavf3as5LyhD+MBWD3BYX5Ec7471cAyG/fmdRpnaX6pBZfJEBq8etU8/7oRpv2oYVDfPrKV5PyzivHATDxtieTuo63CiP2pL6pxRcJkBJfJEDq6teRiY+8lpSPnhlds3crnNzLb3kxKU969WBUp+59kNTiiwRILX4d6dywOSkfev97Acg3FV7PnXN2Us5v2lq1uKT29Njim9mPzGyvmW0oqhttZo+a2db4cVRlwxSRciqlq/+fwIIT6hYDy919OrA8fi4iGdFjV9/d/9fMppxQ/VHgsrh8J7ASuLGMcUkfDBhSGIXXEd97c+ZdhRN62z9b6Oqfoa5+0Pp6cm+8u++Ky7uB8WWKR0SqoN9n9T1adbPbId1mtsjM1pnZunZOPQOMiFRHX8/q7zGzCe6+y8wmAHu729DdlwBLAIbbaN3zUUGvLJqVlCc8dhiAjl27k7qmV6dWPSapTX1t8R8CPh+XPw/8qjzhiEg19Njim9ndRCfyxprZTuAfgJuB+8xsIfAycE0lg5TSeNGfcXv82ZNeb2gr6nBd+CfR49rnKhyV1KJSzupf281LWu9aJKM0ZFckQBqyW0dyR4vKM6YBbx+aO/zuNdUOSWqUWnyRAKnFjx36zNykPOu6ZwDIWbauPk5nfVLu/MLx23Gbut5YElsOnZaUmz4fzVzU0fpKWuFUhVp8kQAp8UUCpK5+bPi2wpmx57/1rhQjkWrLNxfav6bGPSlGUj1q8UUCpMQXCZC6+scVTUrZ+Mi6FAORasuNG5eUfezIFCOpHrX4IgFSix9rG92YlHXlOyz5ffuS8oGrohGPo+p8hiK1+CIBUuKLBEhdfZEibj1vUw/U4osESIkvEqBSVtKZbGYrzGyjmf3RzK6L67WajkhGldLidwDfdPeZwFzgK2Y2E62mI5JZpcy5twvYFZffMLNNwETqYDWdAcOGJeVcW2eKkUitGHg0moNhwODBSV3nkSNphVMxvTqrHy+lNRtYQ4mr6ZjZImARQBODu9pERKqs5MQ3s6HAL4Cvu/vrVjS23d3drOvpamp5QY19nzwvKY9Z+niKkUitGHbPagDe/PhFSd3gB+pvrsKSzuqb2UCipL/L3R+Iq/fEq+jQ02o6IlJbSjmrb8AdwCZ3/17RS1pNRySjSunqXwJ8DnjOzJ6O6/4WraYjklmlnNX/A9DdQEatpiOSQRq5JxIgJb5IgJT4IgEK8rZca4h2e+DhmhpWIDVkwLH6/t1Qiy8SICW+SICC7OrnTo9uKxi9akdS15FWMFKTmncXbszxAbmo0JlPKZryU4svEiAlvkiAlPgiAVLiiwRIiS8SICW+SICU+CIBCvI6fufYEQBY674etpRQDXjjrcKTd5wFQH7riylFU35q8UUCFGSLv29O1OKPeXpjypFIrcpvaUnKrTe+F4CJtwTU4ptZk5mtNbNn4pV0vhPXTzWzNWbWYmb3mlljT/+WiNSGUrr6bcA8d58FnA8sMLO5wC3A9919GnAAWFi5MEWknHpMfI+8GT8dGP84MA+4P66/E/hYRSIUkbIrdV79XDzD7l7gUeAF4KC7H7+pbSfRslpdvXeRma0zs3XttJUjZhHpp5IS393z7n4+MAm4EJhR6ge4+xJ3n+PucwYyqI9hikg59epynrsfBFYAFwMjzez4VYFJQGuZYxORCinlrP44MxsZl5uB+cAmoj8AV8ebaSUdkQwp5Tr+BOBOM8sR/aG4z92XmdlG4B4z+yfgKaJltmpWw6TCKYhhOzXfjpRuxLZoCfXc2DFJXf7V/WmFUxalrKTzLNHS2CfWv0j0fV9EMkZDdkUCFMyQ3TdnF7r6Tb9em2IkkjVD71sNQMfFs5I6y3hXXy2+SICU+CIBUuKLBEiJLxIgJb5IgJT4IgEK5nJerq0z7RAk4zqbckk5d4rtskAtvkiAlPgiAQqmq9/wRnvaIUjGtQ8tpIu6+iKSOUp8kQAF09UX6S/Le9ohlI1afJEA1WWL70W3T7581WAAjo0rzLrTeNXFVY9Jss+LssXeW/gdmrY0mm6y46Xt1Q6pz0pu8eMptp8ys2Xxc62kI5JRvenqX0c0yeZxWklHJKNK6uqb2STgKuCfgevNzIhW0vl0vMmdwLeB2ysQY6/tmTskKW/54r+nGImE4IrlXwQgV4dd/R8ANwDHB7yPQSvpiGRWKfPqfxjY6+7r+/IBWklHpPaU0tW/BPiImV0JNAHDgVuJV9KJW32tpCOSIaWslnuTu09y9ynAp4Dfu/tn0Eo6IpnVnwE8NxKd6Gsh+s5f0yvpiEhBrwbwuPtKYGVc1ko6IhmlIbsiAVLiiwRIiS8SICW+SICU+CIBUuKLBKgu78e3fNoRSL3Le9E6DRmcmEctvkiA6r7Fn3n7l0+qE+mvo2cUfqHekT+WYiR9oxZfJEBKfJEA1WVXv9iZ/xpNI+BtmgREyqfz0tlph9AvavFFAqTEFwlQXXb1O4v3qjODF1ml5lm+cB3/2MhoZvksTSynFl8kQKVOr/0S8AaQBzrcfY6ZjQbuBaYALwHXuPuByoTZO29c8FZSPr09e9dYpfbZ488m5dNWjQbg0G/Siqb3etPi/7m7n+/uc+Lni4Hl7j4dWB4/F5EM6E9X/6NEC2kQP36s/+GISDWUmvgO/NbM1pvZorhuvLvvisu7gfFlj66PLOfJj0hFuCc/QxqOMaQhW18pSz2r/z53bzWz04BHzWxz8Yvu7mbWZZbFfygWATQxuF/Bikh5lNTiu3tr/LgXeJBodt09ZjYBIH7c2817tZKOSI0pZQmtIWY27HgZ+ACwAXiIaCEN0IIaIplSSld/PPBgtEAuDcDP3P1hM3sCuM/MFgIvA9dULkwRKaceEz9eOGNWF/X7gcsrEVRf5MaflpQ73xyYYiQSmpfeiK7jN48qpFP+QE0MaemWRu6JBEiJLxKgurlJJ/+zxqR8zoKnk7Ku5EulDfrwbgA23Vr4RvzOv1qbVjglUYsvEqC6afGbG9qT8lHdmCNVlMzulKGRomrxRQKkxBcJkBJfJEBKfJEAZf7knjVEu7DnyNCkbjh70gpHQpa3tCMomVp8kQAp8UUClPmuPrPOAWDQvxVP8vFCOrFI0Ia2FKWTxd1+r81r+2rxRQKkxBcJUPa7+nGXyjp72E6kwrqedbI2qcUXCVBJiW9mI83sfjPbbGabzOxiMxttZo+a2db4cVSlgxWR8ii1xb8VeNjdZxBNw7UJraQjklmlzLI7Ang/cAeAux9z94NoJR2RzCrl5N5UYB/wYzObBawHrqNGVtJpG9sMQK4tn8bHiyQajhTO7uXeMQWAfMu2lKI5tVK6+g3ABcDt7j4bOMwJ3Xp3d7qZ5crMFpnZOjNb105bf+MVkTIw72FkkZmdDqx29ynx80uJEn8acJm774pX0lnp7uec6t8abqP9IivvjNznrY/+dm14t67nSe34x21PAPCtqe+p6ueu8eW87q/1eLdQjy2+u+8GdpjZ8aS+HNiIVtIRyaxSB/B8FbjLzBqBF4EvEP3R0Eo6IhlUUuK7+9PAnC5eSn0lnQFZGi4lwcjV+MTuGrknEiAlvkiAlPgiAVLiiwQok7flNkw8Iyk/trcJgKG8mFY4Iie57+CFAOTGjE7q8vtfSyuck6jFFwmQEl8kQJns6m9aPDkpT1+wJsVIRLr2zNxBAOz50oyk7rTbHksrnJOoxRcJUCZbfLKzYIlITVKLLxIgJb5IgJT4IgFS4osEKJMn9+yYzu5JjeuMbsttH5ZyHN1Qiy8SICW+SIB67OrHc+3dW1R1NvD3wE/i+inAS8A17n6g/CGebNB+/b3qSW7mOwE4Onl4Ute8trB8eP5AF4dqQC567NRU5f3l7ccAOHre0ZQj6Vopk21ucffz3f184N3AEeBBtJKOSGb1tum8HHjB3V9GK+mIZFZvz+p/Crg7LtfESjrStc1fjtYwvWHesqTu1/NnFTbooqvf8pM/BWDq0sJVk9zKJysUoaSp5BY/nlr7I8DPT3xNK+mIZEtvWvwPAU+6+574+R4zm1C0ks7ert7k7kuAJRCtpNOvaE3X70vluei/esagXUndgxPnFTZofSV6LPo/bWyJ1iGcfPNzSd0rcysYpKSmN9/xr6XQzQetpCOSWSUlvpkNAeYDDxRV3wzMN7OtwBXxcxHJgFJX0jkMjDmhbj9VXknn8MejCQyHba/tVUrScvgTFyXlOz/4HwA8c/SspK5hV+GEXkf8OKC5Oal76/So9nBHYwWjDEvTxsL/b/sV707KA3+3Po1wEhoJIxKgHpfJLqf+LpP9tZbNAPxw2owetgzTN1o2JeUnj0wB4OoRhctxz7VNSMq3L7oagHk//L+k7q57omMz5UeFEX4du/cg5dH8P4Ur3kf/rDL/r2VbJltE6o8SXyRAmbwfX7qWL5qFdOmqywBoe1/hEP/XikuT8u1L7wDg1vlXJnWTtkXTP3cg9U4tvkiAlPgiAVJXv4589bFPF57EF2vaO3NJ1bTrn0jK3169EIDhO9ZVJTapLWrxRQJU8y1+7pxpSflrj88GYBpPpRVOTZv+l4Vr9m8+fDYAHxi+Ial77EMLk/Kwe1YD3dxSKRXxzKbCKMpzx0anUPOv7k8lFrX4IgFS4osEqOa7+vsuGZeUZ3yrFQAfPry7zSX243N/CsDCr1+f1C2+9c6kfNuqC6seU+gmPVIYZ9E5JR4+ra6+iFRLzbf4Xrgaxb+svO+k13d0jKxiNLWt3RuKytHf9G9/d2lSd3bDoaT8lSfXVi8wAWCI/SEp3/zJT59iy8pTiy8SICW+SIBK6uqb2TeALxFd9n0O+AIwAbiHaGae9cDn3P1YuQMc2VKYmfcvVv01AO6FkyTn3hhPGlnFeQVqTjxh5ldXrUiqbpj7sbe9BpC/q/C96eAdkwEYs3xbFQKUE/neTT1vVEE9tvhmNhH4GjDH3c8DckTz698CfN/dpwEHgIXd/ysiUktK7eo3AM1m1gAMBnYB84D749e1ko5IhvTY1Xf3VjP7LrAdOAr8lqhrf9Ddj9+6vROYWIkAcysKw1CnrTj5dd07XrC9vTAfaseUeJqn1c8mdblPjErKo/Mbo+1ef706wUlNKaWrP4ponbypwBnAEGBBqR+glXREak8pJ/euALa5+z4AM3sAuAQYaWYNcas/CWjt6s1lXUlHTumXl56blAe8+TwAnUWvd7k0tgSplO/424G5ZjbYzIxoLv2NwArg6ngbraQjkiE9Jr67ryE6ifck0aW8AUQt+I3A9WbWQnRJ744KxikiZZSpefVF5NQ0r76IdEuJLxIgJb5IgJT4IgGq6sk9M9sHHAZerdqHVt5YtD+1qp72BUrbn7PcfVwP21Q38QHMbJ27z6nqh1aQ9qd21dO+QHn3R119kQAp8UUClEbiL0nhMytJ+1O76mlfoIz7U/Xv+CKSPnX1RQJU1cQ3swVmtsXMWsxscTU/u7/MbLKZrTCzjWb2RzO7Lq4fbWaPmtnW+HFUT/9WLTGznJk9ZWbL4udTzWxNfIzuNbPGtGMslZmNNLP7zWyzmW0ys4uzfHzM7Bvx79oGM7vbzJrKdXyqlvhmlgNuAz4EzASuNbOZ1fr8MugAvunuM4G5wFfi+BcDy919OrA8fp4l1wHFMz9meS7FW4GH3X0GMItovzJ5fCo+16W7V+UHuBh4pOj5TcBN1fr8CuzPr4D5wBZgQlw3AdiSdmy92IdJRMkwD1gGGNEAkYaujlkt/wAjgG3E562K6jN5fIimstsBjCaaMGcZ8MFyHZ9qdvWP78hxFZunr9LMbAowG1gDjHf3XfFLu4HxKYXVFz8AbqAwUc8YqjSXYgVMBfYBP46/uiw1syFk9Pi4eytwfK7LXcAhyjjXpU7u9ZKZDQV+AXzd3d82U6VHf4YzcZnEzD4M7HX39WnHUiYNwAXA7e4+m2ho+Nu69Rk7Pv2a67In1Uz8VmBy0fNu5+mrVWY2kCjp73L3B+LqPWY2IX59ArA3rfh66RLgI2b2EtHCKPOIviOPjKdRh2wdo53ATo9mjIJo1qgLyO7xSea6dPd24G1zXcbb9Pn4VDPxnwCmx2clG4lOVDxUxc/vl3i+wTuATe7+vaKXHiKacxAyNPegu9/k7pPcfQrRsfi9u3+GjM6l6O67gR1mdk5cdXxuyEweHyo912WVT1hcCTwPvAD8XdonUHoZ+/uIuonPAk/HP1cSfS9eDmwFfgeMTjvWPuzbZcCyuHw2sBZoAX4ODEo7vl7sx/nAuvgY/RIYleXjA3wH2AxsAH4KDCrX8dHIPZEA6eSeSICU+CIBUuKLBEiJLxIgJb5IgJT4IgFS4osESIkvEqD/B8g40aYti1/uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newimg = preprocessing(img)[0]\n",
    "plt.imshow(newimg)\n",
    "obs_size = preprocessing(img)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking frames\n",
    "stack_size = 4\n",
    "stacked_frames= deque([np.zeros((84, 84), dtype = np.uint8) for i in range(stack_size)], maxlen = 4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocessing(state)[0]\n",
    "    if is_new_episode:\n",
    "        # Reinitialize with 4 repeated frames\n",
    "        for _ in range(4):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        # Just append the new frame(The last frame will get removed)\n",
    "        stacked_frames.append(frame)\n",
    "    \n",
    "    # Converting Frames Deque to numpy.stack\n",
    "    stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "alpha = 0.0001\n",
    "gamma = 0.99\n",
    "num_epochs = 1000  # Total epochs for training \n",
    "batch_size = 1000 # Each 1 is a timestep (NOT AN EPISODE) --- Just one SARSA tuple\n",
    "\n",
    "state_size = [*obs_size,4]\n",
    "stack_size = 4\n",
    "action_size = env.action_space.n\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                # We create the placeholders\n",
    "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "                # [None, 84, 84, 4]\n",
    "                self.inputs_= tf.placeholder(tf.float32, [None, *state_size], name=\"inputs_\")\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\n",
    "            \n",
    "                \n",
    "                # Add this placeholder for having this variable in tensorboard\n",
    "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\n",
    "                \n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                \"\"\"\n",
    "                First convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                # Input is 84x84x4\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                             filters = 32,\n",
    "                                             kernel_size = [8,8],\n",
    "                                             strides = [4,4],\n",
    "                                             padding = \"VALID\",\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name = \"conv1\")\n",
    "\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm1')\n",
    "\n",
    "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "                ## --> [20, 20, 32]\n",
    "            \n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                \"\"\"\n",
    "                Second convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                     filters = 64,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv2\")\n",
    "\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm2')\n",
    "\n",
    "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "                ## --> [9, 9, 64]\n",
    "            \n",
    "            with tf.name_scope(\"conv3\"):\n",
    "                \"\"\"\n",
    "                Third convnet:\n",
    "                CNN\n",
    "                BatchNormalization\n",
    "                ELU\n",
    "                \"\"\"\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                     filters = 128,\n",
    "                                     kernel_size = [4,4],\n",
    "                                     strides = [2,2],\n",
    "                                     padding = \"VALID\",\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                     name = \"conv3\")\n",
    "\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                       training = True,\n",
    "                                                       epsilon = 1e-5,\n",
    "                                                         name = 'batch_norm3')\n",
    "\n",
    "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "                ## --> [3, 3, 128]\n",
    "            \n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "                ## --> [1152]\n",
    "            \n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name=\"fc1\")\n",
    "            \n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc, \n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              units = action_size, \n",
    "                                            activation=None)\n",
    "            \n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                \n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\n",
    "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the PGNetwork\n",
    "PGNetwork = PGNetwork(state_size, action_size, alpha)\n",
    "\n",
    "# Initialize Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-11f7357cdf3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Setup TensorBoard Writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tensorboard/pg/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPGNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards_of_batch = [] \n",
    "    discounted_rewards = []\n",
    "    rewards_of_episode = []\n",
    "    episode_num = 1 #Initialising episode count\n",
    "    state = env.reset()\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    while True:\n",
    "        action_prob_dist = sess.run(PGNetwork.action_distribution, feed_dict = {PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        # Accounting for Stochastic Policy\n",
    "        chosen_action = random.randint(0,action_size-1)\n",
    "        action_num = np.random.choice(range(action_prob_dist.shape[1]), \n",
    "                                  p=action_prob_dist.ravel())\n",
    "        action_tup = np.zeros(action_size)\n",
    "        action_tup[action_num] = 1\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action_num)\n",
    "        \n",
    "        # STORE\n",
    "        states.append(state)\n",
    "        actions.append(action_tup)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            next_state = np.zeros((250, 160, 3), dtype=np.uint8)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            \n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            # Calculate gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset the transition stores\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            # Add episode\n",
    "            episode_num += 1\n",
    "            state = env.reset()\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "    \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Load the model\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gather training data\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "\n",
    "        ### These part is used for analytics\n",
    "        # Calculate the total reward ot the batch\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the mean reward of the batch\n",
    "        # Total rewards of batch / nb episodes in that batch\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        # Calculate the average reward of all training\n",
    "        # mean_reward_of_that_batch / epoch\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        # Calculate maximum reward recorded \n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"-----------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        # Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                    })\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84, 84, 4)),\n",
    "                                                            PGNetwork.actions: actions_mb,\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\n",
    "                                                                    })\n",
    "\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save Model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    state = env.reset()\n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        total_reward = 0\n",
    "        # Launch a new episode\n",
    "        state = env.reset()\n",
    "        # Get a new state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while True:\n",
    "        \n",
    "            # Run State Through Policy & Calculate Action\n",
    "            action_probability_distribution = sess.run(PGNetwork.action_distribution, \n",
    "                                                       feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "\n",
    "            # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\n",
    "            # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\n",
    "            #30% chance that we take action a2)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), \n",
    "                                      p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "            # action = possible_actions[action]\n",
    "\n",
    "            # Perform action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward+=reward\n",
    "            plt.imshow(next_state)\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                # If not done, the next_state become the current state\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "\n",
    "        print(\"Score for episode \", i, \" :\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
